{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation our model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is a quick evaluation of our model performance. Note that if you trained the model yourself locally there are likely to be small differences between the model you got from your training run and the model whose performance is being evaluated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You probably don't need this.\n",
    "# import t4\n",
    "# t4.Bucket(\"s3://alpha-quilt-storage\").fetch(\n",
    "#     \"aleksey/sagemaker/alekseylearn/alekseylearn-notebooks-build-8/output/model.tar.gz\",\n",
    "#     \"../models/model.tar.gz\"\n",
    "# )\n",
    "# !rm ../models/clf.h5 2>/dev/null\n",
    "# !tar -xvf \"../models/model.tar.gz\"\n",
    "# !mv clf.h5 ../models/clf.h5\n",
    "# !mv history.json ../models/history.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../models/history.json', 'r') as f:\n",
    "    hist = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.6403966411467521,\n",
       "  0.6127266787713573,\n",
       "  0.5962540805339813,\n",
       "  0.6470143996900127,\n",
       "  0.5596090093735726,\n",
       "  0.5438002696851405,\n",
       "  0.536070837128547,\n",
       "  0.5606116306397223,\n",
       "  0.5101688513832707,\n",
       "  0.5737233556086018,\n",
       "  0.5323542361336995,\n",
       "  0.5174307380953143,\n",
       "  0.44509513435825226,\n",
       "  0.5877856698728376,\n",
       "  0.48688628788917293,\n",
       "  0.5060960316076512,\n",
       "  0.5278721720941605],\n",
       " 'val_acc': [0.657258064516129,\n",
       "  0.6471774193548387,\n",
       "  0.7056451612903226,\n",
       "  0.6491935483870968,\n",
       "  0.7157258064516129,\n",
       "  0.7012195126797126,\n",
       "  0.7379032258064516,\n",
       "  0.7278225806451613,\n",
       "  0.7580645161290323,\n",
       "  0.6995967741935484,\n",
       "  0.7296747962633768,\n",
       "  0.7681451612903226,\n",
       "  0.7903225806451613,\n",
       "  0.6935483870967742,\n",
       "  0.7439516129032258,\n",
       "  0.7276422759381737,\n",
       "  0.7258064516129032],\n",
       " 'loss': [0.6867040309229155,\n",
       "  0.6651339889333199,\n",
       "  0.6488449365328374,\n",
       "  0.6274135245850972,\n",
       "  0.6181225183477812,\n",
       "  0.6131593026042554,\n",
       "  0.6059242456533502,\n",
       "  0.5957962876301633,\n",
       "  0.5967469439742288,\n",
       "  0.584502640095624,\n",
       "  0.575604048166929,\n",
       "  0.5800361535385655,\n",
       "  0.5808660827184978,\n",
       "  0.5738759668249833,\n",
       "  0.5655932955954824,\n",
       "  0.5855435873522903,\n",
       "  0.5586245500109793],\n",
       " 'acc': [0.5813397128711286,\n",
       "  0.6064593301435407,\n",
       "  0.6463317384370016,\n",
       "  0.6662679424886688,\n",
       "  0.6614832535409851,\n",
       "  0.6702551833655466,\n",
       "  0.6730462520886836,\n",
       "  0.6897926635719372,\n",
       "  0.6862041467304625,\n",
       "  0.689792663476874,\n",
       "  0.710127591706539,\n",
       "  0.7153110046896257,\n",
       "  0.7212918661237714,\n",
       "  0.7109250397773451,\n",
       "  0.7097288675285413,\n",
       "  0.7101275918016023,\n",
       "  0.7216905900165795]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('../models/clf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    '../data/images_cropped/quilt/open_images/',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "sample_size = len(list(pathlib.Path('../data/images_cropped/').rglob('./*')))\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't iterate automatically using `predict_generator` because it only returns classifications.\n",
    "# It does not report the names of the images being classified. But we need to know these to compare\n",
    "# against the ground truth. See the next code cell for the solution.\n",
    "\n",
    "# y_pred = model.predict_generator(\n",
    "#     validation_generator,\n",
    "#     steps=sample_size // batch_size\n",
    "# )\n",
    "\n",
    "# ...\n",
    "\n",
    "# Following code taken from https://stackoverflow.com/a/49977036/1993206\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "# y_pred = []\n",
    "# image_names = []\n",
    "\n",
    "# for i in tqdm_notebook(range(sample_size // batch_size)):\n",
    "#     indices = next(validation_generator.index_generator)\n",
    "#     images, labels = validation_generator._get_batches_of_transformed_samples(indices)\n",
    "#     for index in indices:\n",
    "#         image_name = validation_generator.filenames[index].split('/')[-1]\n",
    "#         image_names.append(image_name)\n",
    "#     y_pred += labels.astype(int).tolist()\n",
    "\n",
    "# y_pred = np.array(y_pred)\n",
    "\n",
    "# import pandas as pd\n",
    "# X_meta = pd.read_csv('../data/training/X_meta.csv')\n",
    "\n",
    "\n",
    "# y = (X_meta\n",
    "#      .set_index('ImageCropURL')\n",
    "#      .loc[image_names]\n",
    "#      .LabelName\n",
    "#      .map(lambda v: validation_generator.class_indices[v])\n",
    "#      .values\n",
    "#     )\n",
    "\n",
    "\n",
    "# current classification report---two class stopped\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "import os\n",
    "import skimage\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# The following code generates an prediction vector for our input dataset, y_pred\n",
    "# So that we can compare that to our ground truth y.\n",
    "# This is a fast and manual approach, there are better ways.\n",
    "\n",
    "img_uris = []\n",
    "y_pred = []\n",
    "\n",
    "for img_uri in tqdm_notebook(os.listdir('../data/images_cropped/quilt/open_images/Hamburger/')):\n",
    "    img = load_img('../data/images_cropped/quilt/open_images/Hamburger/' + img_uri)\n",
    "    arr = skimage.transform.resize(np.array(img), (128, 128, 3))\n",
    "    y_pred.append(model.predict_classes(arr[np.newaxis, :, :, :])[0][0])\n",
    "    img_uris.append(img_uri)\n",
    "    \n",
    "for img_uri in tqdm_notebook(os.listdir('../data/images_cropped/quilt/open_images/Sandwich/')):\n",
    "    img = load_img('../data/images_cropped/quilt/open_images/Sandwich/' + img_uri)\n",
    "    arr = skimage.transform.resize(np.array(img), (128, 128, 3))\n",
    "    y_pred.append(model.predict_classes(arr[np.newaxis, :, :, :])[0][0])\n",
    "    img_uris.append(img_uri)\n",
    "    \n",
    "y_pred = np.array(y_pred)\n",
    "y = (X_meta\n",
    " .set_index('ImageCropURL')\n",
    " .loc[img_uris]\n",
    " .LabelName\n",
    " .map(lambda v: validation_generator.class_indices[v])\n",
    " .values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.59      0.71      1399\n",
      "           1       0.64      0.92      0.75      1109\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      2508\n",
      "   macro avg       0.77      0.75      0.73      2508\n",
      "weighted avg       0.78      0.73      0.73      2508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could generate further model accuracy improvements by removing depictions and (potentially) occluded images from the dataset. This would likely push average accuracy up to roughly 80%, the same baseline for Cats versus Dogs featured in the article [\"Building powerful image classification models using very little data\"](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html). However we haven't done that here. Exercise left to the reader!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
